{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BertGenerationEncoder' from 'transformers' (/Users/akifumiabe/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-1da9fda11597>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertGenerationEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BertGenerationEncoder' from 'transformers' (/Users/akifumiabe/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import BertGenerationEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>summarize: The Daman and Diu administration on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>summarize: From her special numbers to TV?appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>summarize: The Indira Gandhi Institute of Medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>summarize: Lashkar-e-Taiba's Kashmir commander...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>summarize: Hotels in Mumbai and other Indian c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  summarize: The Daman and Diu administration on...  \n",
       "1  summarize: From her special numbers to TV?appe...  \n",
       "2  summarize: The Indira Gandhi Institute of Medi...  \n",
       "3  summarize: Lashkar-e-Taiba's Kashmir commander...  \n",
       "4  summarize: Hotels in Mumbai and other Indian c...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/news_summary.csv\", encoding=\"latin-1\")\n",
    "df = df[[\"text\", \"ctext\"]]\n",
    "df.ctext = \"summarize: \" + df.ctext\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "SUMMARY_LEN = 150 \n",
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2\n",
    "TRAIN_EPOCHS = 1\n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4\n",
    "SEED = 42\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text = str(df.text)\n",
    "text = ' '.join(text.split())\n",
    "\n",
    "ctext = str(df.ctext)\n",
    "ctext = \" \".join(ctext.split())\n",
    "\n",
    "target = tokenizer.encode_plus(\n",
    "    [text],\n",
    "    None,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN, \n",
    "    pad_to_max_length=True,\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "source = tokenizer.encode_plus(\n",
    "    [ctext],\n",
    "    None,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN, \n",
    "    pad_to_max_length=True,\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "target_ids = target[\"input_ids\"]\n",
    "target_mask = target[\"attention_mask\"]\n",
    "target_token_type_ids = target[\"token_type_ids\"]\n",
    "print(target_ids)\n",
    "print(target_mask)\n",
    "print(target_token_type_ids)\n",
    "\n",
    "source_ids = source[\"input_ids\"]\n",
    "source_mask = source[\"attention_mask\"]\n",
    "source_token_type_ids = source[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataframe, \n",
    "        tokenizer, \n",
    "        source_len, \n",
    "        summ_len\n",
    "    ):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.text\n",
    "        self.ctext = self.data.ctext\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.encode_plus(\n",
    "            [ctext], \n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.source_len, \n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        target = self.tokenizer.encode_plus(\n",
    "            [text],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.summ_len, \n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        target_ids = target[\"input_ids\"]\n",
    "        target_mask = target[\"attention_mask\"]\n",
    "        target_token_type_ids = target[\"token_type_ids\"]\n",
    "\n",
    "        source_ids = source[\"input_ids\"]\n",
    "        source_mask = source[\"attention_mask\"]\n",
    "        source_token_type_ids = source[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            \"source_ids\": torch.tensor(source_ids, dtype=torch.long), \n",
    "            \"source_mask\": torch.tensor(source_mask, dtype=torch.long),\n",
    "            \"source_token_type_ids\": torch.tensor(source_token_type_ids, dtype=torch.long),\n",
    "            \"target_ids\": torch.tensor(target_ids, dtype=torch.long), \n",
    "            \"target_mask\": torch.tensor(target_mask, dtype=torch.long),\n",
    "            \"target_token_type_ids\": torch.tensor(target_token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (4514, 2)\n",
      "TRAIN Dataset: (3611, 2)\n",
      "TEST Dataset: (903, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = df.sample(frac=train_size, random_state=SEED)\n",
    "val_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(f\"FULL Dataset: {df.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "print(f\"TEST Dataset: {val_dataset.shape}\")\n",
    "\n",
    "training_set = CustomDataset(\n",
    "    train_dataset, \n",
    "    tokenizer, \n",
    "    MAX_LEN, \n",
    "    SUMMARY_LEN\n",
    ")\n",
    "    \n",
    "val_set = CustomDataset(\n",
    "    val_dataset, \n",
    "    tokenizer, \n",
    "    MAX_LEN, \n",
    "    SUMMARY_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "val_params = {\n",
    "        \"batch_size\": VALID_BATCH_SIZE,\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_set, \n",
    "    **train_params\n",
    ")\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    **val_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), \n",
    "    lr=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.4108,  -6.3915,  -6.3496,  ...,  -5.7395,  -5.7290,  -3.7498],\n",
      "         [ -9.9286, -10.0836,  -9.8449,  ...,  -7.9650,  -8.2284,  -3.9248],\n",
      "         [-11.7743, -11.4326, -11.5535,  ..., -10.4621,  -9.9159,  -6.0488],\n",
      "         ...,\n",
      "         [ -4.0528,  -4.1600,  -3.9803,  ...,  -4.1490,  -5.0253,   0.0801],\n",
      "         [ -3.5102,  -3.2376,  -3.4060,  ...,  -2.8583,  -3.7053,   0.4102],\n",
      "         [ -7.2019,  -7.1214,  -7.1733,  ...,  -6.8107,  -6.7348,  -3.1668]],\n",
      "\n",
      "        [[ -7.0004,  -6.8637,  -6.9108,  ...,  -6.2522,  -6.3393,  -3.7306],\n",
      "         [-11.1964, -11.1939, -11.1958,  ...,  -9.7810, -10.3395,  -4.5830],\n",
      "         [ -8.2854,  -8.2554,  -8.4749,  ...,  -4.9942,  -5.9247,  -7.8574],\n",
      "         ...,\n",
      "         [ -5.2403,  -5.0797,  -5.1067,  ...,  -5.2289,  -5.7570,  -0.8729],\n",
      "         [ -6.2895,  -6.0007,  -6.1122,  ...,  -5.5002,  -6.6823,  -0.3538],\n",
      "         [ -6.8295,  -6.7400,  -6.8097,  ...,  -6.1811,  -6.4679,  -1.7245]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ -6.5037,  -6.3766,  -6.3378,  ...,  -5.6718,  -5.2954,  -3.7025],\n",
      "         [ -9.0244,  -9.4707,  -9.2424,  ...,  -7.7202,  -8.4957,  -3.4147],\n",
      "         [ -6.2798,  -6.4242,  -6.2081,  ...,  -5.5326,  -5.2632,  -5.0124],\n",
      "         ...,\n",
      "         [ -3.1666,  -3.1264,  -3.0177,  ...,  -3.5119,  -3.2817,  -0.9124],\n",
      "         [ -4.7936,  -4.7086,  -4.7188,  ...,  -4.0527,  -5.0754,  -0.2834],\n",
      "         [ -5.6738,  -5.6752,  -5.6341,  ...,  -5.1073,  -4.4932,  -2.5299]],\n",
      "\n",
      "        [[ -5.9048,  -5.7803,  -5.7565,  ...,  -5.2962,  -5.1042,  -3.2958],\n",
      "         [-10.1196, -10.1559, -10.2315,  ...,  -8.1950,  -9.6260,  -2.7232],\n",
      "         [-11.2116, -10.8124, -11.0818,  ..., -11.3294, -10.2481,  -8.9669],\n",
      "         ...,\n",
      "         [ -4.2041,  -4.1027,  -4.0916,  ...,  -4.5014,  -5.8277,   0.4674],\n",
      "         [ -4.7273,  -4.5269,  -4.6568,  ...,  -4.3411,  -6.1201,   0.0748],\n",
      "         [ -5.7475,  -5.6149,  -5.7275,  ...,  -5.7362,  -6.1285,   0.3012]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-6.8035e+00, -6.7223e+00, -6.7235e+00,  ..., -6.0623e+00,\n",
      "          -5.9633e+00, -3.9313e+00],\n",
      "         [-9.4894e+00, -9.4976e+00, -9.4898e+00,  ..., -7.4028e+00,\n",
      "          -8.5707e+00, -1.9870e+00],\n",
      "         [-8.4267e+00, -8.5779e+00, -8.0149e+00,  ..., -6.7325e+00,\n",
      "          -5.9756e+00, -8.1577e+00],\n",
      "         ...,\n",
      "         [-5.1152e+00, -5.1003e+00, -5.0812e+00,  ..., -4.7707e+00,\n",
      "          -4.1378e+00, -7.2964e-01],\n",
      "         [-5.7884e+00, -5.6043e+00, -5.7243e+00,  ..., -5.1818e+00,\n",
      "          -5.5606e+00, -2.6419e-02],\n",
      "         [-5.6483e+00, -5.6921e+00, -5.7190e+00,  ..., -5.0737e+00,\n",
      "          -4.7703e+00,  6.3964e-01]],\n",
      "\n",
      "        [[-6.1494e+00, -6.1188e+00, -6.1285e+00,  ..., -5.5314e+00,\n",
      "          -5.3347e+00, -3.3776e+00],\n",
      "         [-9.4875e+00, -9.8006e+00, -9.7033e+00,  ..., -7.4129e+00,\n",
      "          -8.3461e+00, -2.6504e+00],\n",
      "         [-1.1180e+01, -1.1219e+01, -1.0975e+01,  ..., -1.0420e+01,\n",
      "          -8.7123e+00, -8.2076e+00],\n",
      "         ...,\n",
      "         [-6.0403e+00, -6.0495e+00, -6.1061e+00,  ..., -4.7999e+00,\n",
      "          -5.5512e+00, -3.2090e-01],\n",
      "         [-5.5351e+00, -5.4885e+00, -5.5342e+00,  ..., -4.6827e+00,\n",
      "          -6.0119e+00,  9.5850e-03],\n",
      "         [-5.6501e+00, -5.6171e+00, -5.6924e+00,  ..., -5.4713e+00,\n",
      "          -5.5559e+00, -1.1623e+00]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[ -6.2135,  -6.1234,  -6.1501,  ...,  -5.5943,  -5.3838,  -3.6876],\n",
      "         [-10.3524, -10.4274, -10.5288,  ...,  -9.6909,  -9.5031,  -2.5710],\n",
      "         [ -9.5128,  -9.2763,  -9.3196,  ...,  -7.6728,  -7.6081,  -4.8719],\n",
      "         ...,\n",
      "         [ -4.4281,  -4.1008,  -4.1689,  ...,  -3.7951,  -4.4823,   0.3146],\n",
      "         [ -3.9026,  -3.9985,  -3.9145,  ...,  -4.0403,  -4.5190,   0.3847],\n",
      "         [ -6.3181,  -6.3338,  -6.3221,  ...,  -5.4701,  -5.7196,  -1.9418]],\n",
      "\n",
      "        [[ -6.5314,  -6.4715,  -6.4569,  ...,  -5.8488,  -5.7162,  -3.7350],\n",
      "         [-10.3017, -10.3631, -10.4411,  ...,  -9.7418, -11.0597,  -4.6575],\n",
      "         [-10.9826, -10.6468, -10.6756,  ...,  -9.4846,  -9.6889,  -4.6555],\n",
      "         ...,\n",
      "         [ -4.3731,  -4.1939,  -4.2446,  ...,  -4.6062,  -4.6756,  -0.6537],\n",
      "         [ -4.2439,  -4.2007,  -4.1310,  ...,  -4.9413,  -5.6187,   0.6626],\n",
      "         [ -4.5020,  -4.2583,  -4.3363,  ...,  -4.7036,  -4.6058,  -1.5838]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ -8.2288,  -8.1156,  -8.1265,  ...,  -7.4784,  -7.3608,  -4.1413],\n",
      "         [ -8.8538,  -8.7371,  -9.0342,  ...,  -7.7428,  -9.2453,  -2.6814],\n",
      "         [-10.6145, -10.5944, -10.3772,  ...,  -7.7005,  -8.7379,  -9.5421],\n",
      "         ...,\n",
      "         [ -5.2055,  -5.1087,  -5.1603,  ...,  -5.1372,  -5.4930,  -0.4044],\n",
      "         [ -3.8205,  -3.8401,  -3.8021,  ...,  -3.7287,  -4.4530,   0.0344],\n",
      "         [ -6.9580,  -6.9513,  -6.9756,  ...,  -7.1259,  -6.8712,  -2.8287]],\n",
      "\n",
      "        [[ -6.0944,  -6.0129,  -6.0159,  ...,  -5.5149,  -5.3886,  -3.3998],\n",
      "         [ -6.3392,  -6.2911,  -6.5353,  ...,  -4.3447,  -5.3662,  -1.6609],\n",
      "         [-12.3277, -12.0169, -12.0909,  ...,  -9.1277, -10.4825,  -9.0882],\n",
      "         ...,\n",
      "         [ -4.3647,  -4.4085,  -4.3321,  ...,  -4.6519,  -4.3797,  -0.4879],\n",
      "         [ -3.9335,  -3.6484,  -3.5776,  ...,  -3.6643,  -3.9090,   1.2593],\n",
      "         [ -2.9791,  -2.8863,  -3.0172,  ...,  -2.5052,  -2.5087,   1.2304]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ -6.6264,  -6.5307,  -6.5106,  ...,  -5.8389,  -5.5078,  -4.0135],\n",
      "         [-10.4265, -10.4077, -10.6083,  ...,  -9.0840,  -9.7543,  -2.6862],\n",
      "         [ -9.5205,  -9.3457,  -9.5213,  ...,  -6.5213,  -7.2501,  -7.1900],\n",
      "         ...,\n",
      "         [ -5.7031,  -5.6294,  -5.6326,  ...,  -6.1409,  -6.5284,  -0.8803],\n",
      "         [ -5.5903,  -5.5585,  -5.5533,  ...,  -5.1752,  -5.8524,  -1.5280],\n",
      "         [ -5.5321,  -5.6835,  -5.7521,  ...,  -5.0216,  -5.9575,  -0.1598]],\n",
      "\n",
      "        [[ -6.6947,  -6.6357,  -6.6614,  ...,  -5.9624,  -5.6986,  -3.8882],\n",
      "         [ -8.4894,  -8.3918,  -8.6548,  ...,  -8.2698,  -9.3610,   0.7350],\n",
      "         [ -8.0722,  -7.8651,  -7.9992,  ...,  -6.1297,  -7.0176,  -8.1591],\n",
      "         ...,\n",
      "         [ -5.9838,  -5.9830,  -5.9344,  ...,  -5.8774,  -5.7569,  -1.0995],\n",
      "         [ -4.1368,  -4.0536,  -4.0125,  ...,  -3.8986,  -4.4400,   0.6357],\n",
      "         [ -5.7400,  -5.6702,  -5.7380,  ...,  -5.1272,  -5.9218,  -0.2085]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ -6.2491,  -6.1000,  -6.1145,  ...,  -5.5590,  -5.4783,  -3.4832],\n",
      "         [ -9.3072,  -9.2196,  -9.6694,  ...,  -7.8084,  -9.4130,  -5.0062],\n",
      "         [ -9.3016,  -9.1813,  -9.0702,  ...,  -7.4190,  -8.3533,  -7.9703],\n",
      "         ...,\n",
      "         [ -4.2064,  -4.2394,  -4.1268,  ...,  -3.8584,  -4.6621,  -1.5263],\n",
      "         [ -6.5286,  -6.4704,  -6.5700,  ...,  -5.2775,  -6.2745,  -3.8687],\n",
      "         [ -6.6998,  -6.7246,  -6.7848,  ...,  -6.1572,  -6.3601,  -2.3551]],\n",
      "\n",
      "        [[ -6.2650,  -6.1881,  -6.2065,  ...,  -5.5913,  -5.4486,  -3.6998],\n",
      "         [-12.4614, -12.6552, -12.3901,  ..., -10.0269, -10.6400,  -5.5962],\n",
      "         [-11.0309, -10.6184, -10.6247,  ...,  -9.1334,  -9.0799,  -9.6547],\n",
      "         ...,\n",
      "         [ -1.9429,  -2.1999,  -2.0268,  ...,  -4.0448,  -3.4242,   1.0090],\n",
      "         [ -4.9975,  -4.8192,  -4.8215,  ...,  -4.1806,  -6.0602,  -0.1938],\n",
      "         [ -6.3124,  -6.2122,  -6.1762,  ...,  -5.9967,  -6.5278,  -1.6525]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ -6.1616,  -6.1021,  -6.0591,  ...,  -5.3456,  -5.3962,  -3.0624],\n",
      "         [ -8.8189,  -8.6504,  -8.6923,  ...,  -6.5919,  -8.5832,   0.2689],\n",
      "         [ -8.6342,  -8.7803,  -8.3364,  ...,  -6.8468,  -7.7868,  -6.1533],\n",
      "         ...,\n",
      "         [ -1.3668,  -1.8631,  -1.4060,  ...,  -2.4945,  -3.0715,   0.9559],\n",
      "         [ -5.5326,  -5.4924,  -5.4739,  ...,  -5.0414,  -6.9286,  -0.3196],\n",
      "         [ -5.9739,  -5.8585,  -5.9651,  ...,  -5.9245,  -5.8959,  -1.4492]],\n",
      "\n",
      "        [[ -7.1746,  -7.0786,  -6.9835,  ...,  -6.2744,  -6.5713,  -3.5899],\n",
      "         [ -9.8755, -10.0273, -10.1262,  ...,  -8.7087,  -8.0379,  -4.5764],\n",
      "         [-13.5352, -12.9831, -13.1993,  ..., -10.7305,  -9.9263, -10.4408],\n",
      "         ...,\n",
      "         [ -5.2434,  -5.1111,  -5.2479,  ...,  -4.4790,  -5.6286,  -0.9194],\n",
      "         [ -5.8887,  -5.9637,  -5.8850,  ...,  -5.1896,  -5.5504,  -2.5190],\n",
      "         [ -6.9951,  -7.0104,  -6.9779,  ...,  -6.3411,  -6.2548,  -3.8591]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.9111,  -6.7878,  -6.8206,  ...,  -5.9530,  -6.0225,  -3.9293],\n",
      "         [-10.0338, -10.0146, -10.1563,  ...,  -7.9081,  -9.3879,  -3.3911],\n",
      "         [ -9.2153,  -9.3211,  -9.4302,  ...,  -7.8361,  -8.4466,  -6.1223],\n",
      "         ...,\n",
      "         [ -4.7590,  -4.4648,  -4.6052,  ...,  -4.4481,  -5.2512,   0.4812],\n",
      "         [ -5.1831,  -5.1366,  -5.1438,  ...,  -5.3061,  -5.7935,  -0.2311],\n",
      "         [ -6.1292,  -6.0214,  -6.0673,  ...,  -6.0712,  -6.4468,  -1.2113]],\n",
      "\n",
      "        [[ -6.5111,  -6.4546,  -6.4520,  ...,  -5.7688,  -5.6899,  -3.7902],\n",
      "         [ -9.4569,  -9.3786,  -9.5252,  ...,  -8.7242,  -9.4209,  -1.5879],\n",
      "         [ -7.6269,  -7.5002,  -7.5629,  ...,  -5.7900,  -6.4830,  -5.9856],\n",
      "         ...,\n",
      "         [ -5.2929,  -5.2675,  -5.2987,  ...,  -5.2662,  -5.5948,  -1.1058],\n",
      "         [ -6.6667,  -6.4979,  -6.6764,  ...,  -6.1929,  -6.6399,  -0.9622],\n",
      "         [ -7.1871,  -7.1460,  -7.3020,  ...,  -6.5488,  -6.5475,  -2.5243]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ -6.4119,  -6.3297,  -6.3526,  ...,  -5.5736,  -5.5787,  -3.5984],\n",
      "         [ -8.2382,  -7.8634,  -8.1906,  ...,  -7.4418,  -7.9273,  -0.6838],\n",
      "         [-10.2558, -10.4240, -10.4205,  ...,  -8.6193,  -7.6078,  -7.8329],\n",
      "         ...,\n",
      "         [ -2.8771,  -2.7155,  -2.8753,  ...,  -2.8124,  -3.3054,   1.5237],\n",
      "         [ -4.5495,  -4.2568,  -4.4799,  ...,  -3.8128,  -4.7732,   0.5521],\n",
      "         [ -4.4551,  -4.3033,  -4.4497,  ...,  -4.2616,  -4.5967,  -0.4270]],\n",
      "\n",
      "        [[ -6.7987,  -6.7038,  -6.6669,  ...,  -6.2212,  -6.1201,  -3.8419],\n",
      "         [ -8.2695,  -8.1627,  -8.3976,  ...,  -6.3157,  -6.9972,  -2.4489],\n",
      "         [ -9.9918,  -9.6350,  -9.3276,  ...,  -7.9466,  -8.3180,  -7.2083],\n",
      "         ...,\n",
      "         [ -5.6020,  -5.5715,  -5.5530,  ...,  -4.9770,  -5.8907,  -1.2193],\n",
      "         [ -5.8605,  -5.4678,  -5.8429,  ...,  -5.1215,  -5.7238,  -0.5587],\n",
      "         [ -4.8982,  -4.8113,  -4.9304,  ...,  -4.3698,  -4.2528,  -0.3074]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[ -6.6737,  -6.6792,  -6.7011,  ...,  -5.9499,  -6.0253,  -3.7710],\n",
      "         [ -8.9783,  -8.8433,  -8.9776,  ...,  -7.2717,  -8.7940,  -2.0009],\n",
      "         [ -9.9043,  -9.8379,  -9.6751,  ...,  -8.3507,  -7.7945,  -9.2015],\n",
      "         ...,\n",
      "         [  2.7549,   2.2159,   2.5274,  ...,   1.1300,   0.4443,   1.5014],\n",
      "         [ -4.7190,  -4.4241,  -4.7100,  ...,  -3.7868,  -5.2914,  -0.2058],\n",
      "         [ -5.0352,  -4.9884,  -5.0247,  ...,  -4.2844,  -5.3768,  -1.8514]],\n",
      "\n",
      "        [[ -6.3487,  -6.2440,  -6.2542,  ...,  -5.7575,  -5.3012,  -3.5369],\n",
      "         [ -9.5601,  -9.4946,  -9.5912,  ...,  -8.9518,  -8.8189,  -2.3131],\n",
      "         [-10.8646, -10.6638, -10.8219,  ...,  -9.2169,  -8.2506, -10.1142],\n",
      "         ...,\n",
      "         [ -4.7485,  -4.6960,  -4.6765,  ...,  -4.5924,  -5.1786,  -0.4490],\n",
      "         [ -6.0372,  -5.8860,  -5.9785,  ...,  -6.4252,  -6.9392,  -1.8692],\n",
      "         [ -6.5708,  -6.4580,  -6.5849,  ...,  -6.7797,  -6.4699,  -2.4003]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[-6.8581e+00, -6.7626e+00, -6.7329e+00,  ..., -6.1757e+00,\n",
      "          -6.4296e+00, -3.7986e+00],\n",
      "         [-9.2686e+00, -8.9849e+00, -9.2591e+00,  ..., -7.9822e+00,\n",
      "          -9.5076e+00, -4.1820e+00],\n",
      "         [-1.0755e+01, -1.0836e+01, -1.0612e+01,  ..., -8.9662e+00,\n",
      "          -7.4137e+00, -6.5368e+00],\n",
      "         ...,\n",
      "         [-4.3203e+00, -4.1268e+00, -4.2726e+00,  ..., -2.9807e+00,\n",
      "          -5.0463e+00,  4.1960e-01],\n",
      "         [-6.8644e+00, -6.5178e+00, -6.9060e+00,  ..., -6.1602e+00,\n",
      "          -7.3623e+00,  3.2549e-01],\n",
      "         [-5.6392e+00, -5.5394e+00, -5.5390e+00,  ..., -5.0948e+00,\n",
      "          -5.4644e+00, -1.8479e+00]],\n",
      "\n",
      "        [[-6.6791e+00, -6.6103e+00, -6.6593e+00,  ..., -5.9088e+00,\n",
      "          -6.1947e+00, -3.1091e+00],\n",
      "         [-8.8264e+00, -8.7088e+00, -8.6997e+00,  ..., -7.0888e+00,\n",
      "          -7.9377e+00, -1.8016e+00],\n",
      "         [-9.4203e+00, -9.2858e+00, -9.3053e+00,  ..., -8.6308e+00,\n",
      "          -7.5771e+00, -9.0817e+00],\n",
      "         ...,\n",
      "         [-5.4266e+00, -5.0752e+00, -5.3527e+00,  ..., -4.3938e+00,\n",
      "          -5.5542e+00, -2.7319e-02],\n",
      "         [-4.9816e+00, -4.7900e+00, -4.8419e+00,  ..., -4.8672e+00,\n",
      "          -5.9457e+00,  3.9907e-01],\n",
      "         [-5.5569e+00, -5.3700e+00, -5.5208e+00,  ..., -4.7074e+00,\n",
      "          -5.9715e+00,  4.6935e-03]]], grad_fn=<AddBackward0>)\n",
      "tensor([[[-6.0111, -6.0438, -6.0793,  ..., -5.5444, -5.4015, -2.9967],\n",
      "         [-9.1021, -8.6857, -9.2339,  ..., -7.2625, -8.8430, -1.9213],\n",
      "         [-9.7561, -9.9248, -9.3215,  ..., -7.0940, -8.3114, -5.3820],\n",
      "         ...,\n",
      "         [-4.0518, -3.8923, -4.1150,  ..., -3.1653, -4.5452,  0.4362],\n",
      "         [-4.4468, -4.2831, -4.5996,  ..., -3.9447, -5.0004,  0.6445],\n",
      "         [-5.7180, -5.6748, -5.7819,  ..., -5.0401, -5.4643, -0.9006]],\n",
      "\n",
      "        [[-7.1707, -7.0808, -7.0916,  ..., -6.2998, -6.2521, -4.2509],\n",
      "         [-9.3314, -9.3113, -9.6061,  ..., -8.0596, -8.4533, -5.1244],\n",
      "         [-8.9996, -8.7985, -8.5870,  ..., -8.2812, -7.2249, -7.0651],\n",
      "         ...,\n",
      "         [-5.9739, -5.9130, -5.9172,  ..., -5.6442, -5.9134, -2.4085],\n",
      "         [-4.8548, -4.7654, -4.7980,  ..., -4.3829, -5.1492, -0.1290],\n",
      "         [-6.7466, -6.8372, -6.7995,  ..., -6.2618, -5.4039, -3.3842]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-478da638efc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtarget_token_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Unexpected keyword arguments: {list(kwargs.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1074\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         )\n\u001b[0;32m--> 755\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 )\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    434\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cat-in-the-dat-ii/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    model.train()\n",
    "    for _, data in enumerate(training_loader, 0):\n",
    "        source_ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        source_mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "        source_token_type_ids = data[\"source_token_type_ids\"].to(device, dtype=torch.long)\n",
    "        target_ids = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        target_mask = data[\"target_mask\"].to(device, dtype=torch.long)\n",
    "        target_token_type_ids = data[\"target_token_type_ids\"].to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=source_mask,\n",
    "            token_type_ids=source_token_type_ids,\n",
    "            #position_ids=target_ids,\n",
    "            #encoder_attention_mask=target_mask,\n",
    "            #labels=target_token_type_ids,\n",
    "        )\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        print(loss)\n",
    "        \n",
    "        #if _%500==0:\n",
    "        #    print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
